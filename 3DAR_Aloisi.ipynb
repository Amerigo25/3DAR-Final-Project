{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r1urANWETaA"
   },
   "source": [
    "3D Augmented Reality Final Project\n",
    "\n",
    "\n",
    "Local feature compression using autoencoders\n",
    "---\n",
    "**University of Padua**<br>\n",
    "*Master Degree in ICT for Internet and Multimedia - Cybersystems*\n",
    "*Student: Amerigo Aloisi*\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2216,
     "status": "ok",
     "timestamp": 1681749989419,
     "user": {
      "displayName": "Amerigo Aloisi",
      "userId": "09720676480451782874"
     },
     "user_tz": -120
    },
    "id": "IvNuH65fGoXE"
   },
   "outputs": [],
   "source": [
    "# needed imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import os\n",
    "import cv2 as cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-RkiBk5ii55",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# change the option to download the dataset from google drive\n",
    "DOWNLOAD = False\n",
    "if (DOWNLOAD):\n",
    "    !pip install gdown\n",
    "    import gdown\n",
    "    url = 'https://drive.google.com/drive/u/0/folders/1vRlC1Ih5UszyLFX46r2uNC4nhwM_7nY3'\n",
    "    gdown.download_folder(url, quiet=False)\n",
    "    DOWNLOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion to load the images and extract SIFT features from them \n",
    "\n",
    "def load_dataset(datapath):\n",
    "\n",
    "    image_filenames = [img for img in os.listdir(datapath) if os.path.splitext(img)[1].lower() == \".jpg\"]\n",
    "    \n",
    "    sift = cv.SIFT_create()\n",
    "    \n",
    "    descriptors = []\n",
    "\n",
    "    keypoints = []\n",
    "\n",
    "    for image_filename in image_filenames:\n",
    "\n",
    "        image = cv.imread(os.path.join(datapath, image_filename),cv.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # apply pre-processing to improve performance\n",
    "        \n",
    "        clahe = cv.createCLAHE()\n",
    "\n",
    "        image = clahe.apply(image) \n",
    "\n",
    "        kp, des = sift.detectAndCompute(image,None)\n",
    "        \n",
    "        # l2 normalization\n",
    "        \n",
    "        des /= np.linalg.norm(des, axis=1, keepdims=True)\n",
    "    \n",
    "        keypoints.append(kp)\n",
    "    \n",
    "        descriptors.append(des)\n",
    "\n",
    "    return keypoints,descriptors\n",
    "\n",
    "# function to save keypoints to txt file respecting the colmap format\n",
    "\n",
    "def save_keypoints (keypoints,datapath):\n",
    "        \n",
    "    image_filenames = [img for img in os.listdir(datapath) if os.path.splitext(img)[1].lower() == \".jpg\"]\n",
    "\n",
    "    # keypoints are now saved to file\n",
    "\n",
    "    for i, keypoint in enumerate (keypoints):\n",
    "\n",
    "        header = str(np.shape(keypoint)[0]) + \" 128\" # colmap header\n",
    "        \n",
    "        # filename\n",
    " \n",
    "        filename = os.path.join(datapath, image_filenames [i]+\".txt\")\n",
    "    \n",
    "        with open(filename, 'w') as f:\n",
    "            \n",
    "            f.write(header + '\\n')    \n",
    "            \n",
    "        # the cv object keypoint is serialized into a string with 4 values: xposition, yposition, size, angle\n",
    "        \n",
    "            for kp in keypoint:\n",
    "                x, y = kp.pt\n",
    "                size = kp.size\n",
    "                angle = kp.angle\n",
    "                f.write('{} {} {} {}\\n'.format(x, y, size,angle))\n",
    "            \n",
    "       # 128 zeros are added to each line (meaning no descriptor provided)\n",
    "\n",
    "        with open(filename, 'r') as input_file:\n",
    "\n",
    "            file_contents = input_file.read()\n",
    "\n",
    "            lines = file_contents.split('\\n')\n",
    "\n",
    "            modified_lines = [lines[0]] + [line + ' ' + '0 '*128 for line in lines[1:-2]]\n",
    "\n",
    "            modified_file = '\\n'.join(modified_lines)\n",
    "\n",
    "\n",
    "        with open(filename, 'w') as output_file:\n",
    "\n",
    "            output_file.write(modified_file)\n",
    "            \n",
    "# function to find feature matches between each pair of images and then save them to txt files according to Colmap format\n",
    "\n",
    "def save_matches(descriptors,keypoints,datapath,output_file):\n",
    "        \n",
    "    # FLANN matcher for large dataset\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks=50)   # or pass empty dictionary\n",
    "    flann = cv.FlannBasedMatcher(index_params,search_params)\n",
    "    \n",
    "    filenames = [img for img in os.listdir(datapath) if os.path.splitext(img)[1].lower() == \".jpg\"]\n",
    "        \n",
    "    with open(output_file,'w') as f:\n",
    "    \n",
    "        for i in range(len(descriptors)):\n",
    "        \n",
    "            for j in range(i+1,len(descriptors)):\n",
    "            \n",
    "                header = filenames[i] + ' ' + filenames [j]\n",
    "            \n",
    "                sift1 = descriptors[i] \n",
    "                sift2 = descriptors[j] \n",
    "                matches = flann.knnMatch(sift1,sift2,k=2) \n",
    "                \n",
    "                print (\"Number of matches: \", len(matches))\n",
    "                \n",
    "                # Lowe ratio test \n",
    "                good_matches = []\n",
    "                for m, n in matches:\n",
    "                    if m.distance < 0.8 * n.distance:\n",
    "                        good_matches.append(m)\n",
    "                \n",
    "                print(\"Matches after filtering: \", len(good_matches))\n",
    "                \n",
    "                             \n",
    "                \"\"\"\n",
    "                # Geometric verification: filter out some wrong matches but takes much more time\n",
    "                \n",
    "                pts1 = []\n",
    "                pts2 = []\n",
    "\n",
    "                for match in good_matches:\n",
    "                    pts1.append(keypoints[i][match.queryIdx].pt)\n",
    "                    pts2.append(keypoints[j][match.trainIdx].pt)\n",
    "\n",
    "                pts1 = np.int32(pts1)\n",
    "                pts2 = np.int32(pts2)\n",
    "                \n",
    "                F, mask = cv.findFundamentalMat(pts1,pts2,cv.USAC_MAGSAC) \n",
    "\n",
    "                # Apply the epipolar constraint to filter out bad matches\n",
    "                new_matches = []\n",
    "                for k in range(len(good_matches)):\n",
    "                    if mask[k] == 1:\n",
    "                        new_matches.append(good_matches[k])\n",
    "                        \n",
    "                \n",
    "                print(\"Number of inliers: \", len(new_matches))\n",
    "\n",
    "                             \n",
    "                \"\"\"\n",
    "                \n",
    "                # saving\n",
    "                # change good_matches to new_matches if you want to perform geometric validation \n",
    "                  \n",
    "                idx1 = np.array([m.queryIdx for m in good_matches])\n",
    "                idx2 = np.array([m.trainIdx for m in good_matches])\n",
    "                \n",
    "                f.write(header + '\\n')\n",
    "                \n",
    "                for n in range(len(good_matches)):\n",
    "                    f.write('{} {}\\n'.format(idx1[n],idx2[n]))\n",
    "                    \n",
    "                f.write('\\n')\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1681747286723,
     "user": {
      "displayName": "Amerigo Aloisi",
      "userId": "09720676480451782874"
     },
     "user_tz": -120
    },
    "id": "GbKIUS-FNqmK"
   },
   "outputs": [],
   "source": [
    "# load training set\n",
    "# change this with your local path\n",
    "train_path = \"castle\"\n",
    "train_path2 = \"Herz-Jesus-P8\"\n",
    "train_kp,train_des = load_dataset(train_path)\n",
    "train_kp2,train_des2 = load_dataset(train_path2)\n",
    "train_set = np.concatenate((np.concatenate(train_des),np.concatenate(train_des2)), axis=0)\n",
    "print(np.shape(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1681747286723,
     "user": {
      "displayName": "Amerigo Aloisi",
      "userId": "09720676480451782874"
     },
     "user_tz": -120
    },
    "id": "GbKIUS-FNqmK"
   },
   "outputs": [],
   "source": [
    "# Define the encoder layers\n",
    "input_shape = (train_set.shape[1]) \n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder_l1 = Dense(64, activation='relu')(input_layer)\n",
    "encoder_l2 = Dense(32, activation='relu')(encoder_l1)\n",
    "encoder_l3 = Dense(16, activation='relu')(encoder_l2)\n",
    "encoder_l4 = Dense(8, activation='relu')(encoder_l3)\n",
    "\n",
    "\n",
    "# define the encoder model\n",
    "encoder = Model(inputs = input_layer, outputs = encoder_l3, name = 'encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Define the decoder layers\n",
    "decoder_l1 = Dense(16, activation='relu')(encoder_l4)\n",
    "decoder_l2 = Dense(32, activation='relu')(decoder_l1)\n",
    "decoder_l3 = Dense(64, activation='relu')(decoder_l2)\n",
    "output_layer = Dense(input_shape, activation='relu')(decoder_l3)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(input_layer, output_layer, name = 'autoencoder')\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4e92s_ndqgA"
   },
   "outputs": [],
   "source": [
    "# Train the autoencoder on the SIFT features\n",
    "history = autoencoder.fit(train_set, train_set, epochs=10, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "# change this with your local path\n",
    "test_path = \"fountain\"\n",
    "test_path2 = \"santo\"\n",
    "# extract SIFT features test sets\n",
    "test_kp,test_des = load_dataset(test_path)\n",
    "save_keypoints (test_kp,test_path)\n",
    "test_kp2,test_des2 = load_dataset(test_path2)\n",
    "save_keypoints (test_kp2,test_path2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the test sets\n",
    "compressed = []\n",
    "for des in test_des:\n",
    "    comp_des = encoder.predict(des)\n",
    "    compressed.append(comp_des)\n",
    "    \n",
    "compressed2 = []\n",
    "for des in test_des2:\n",
    "    comp_des = encoder.predict(des)\n",
    "    compressed2.append(comp_des)\n",
    "\n",
    "\n",
    "# reconstruct the test sets\n",
    "reconstructed = []\n",
    "for des in test_des:\n",
    "    dec_des = autoencoder.predict(des)\n",
    "    reconstructed.append(dec_des)\n",
    "    \n",
    "reconstructed2 = []\n",
    "for des in test_des2:\n",
    "    dec_des = autoencoder.predict(des)\n",
    "    reconstructed2.append(dec_des)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_matches(test_des,test_kp,test_path, \"original_fountain.txt\")\n",
    "save_matches(reconstructed,test_kp,test_path, \"decoded_fountain.txt\")\n",
    "save_matches(compressed,test_kp,test_path, \"comp_fountain.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_matches(test_des2,test_kp2,test_path2, \"original_santo.txt\")\n",
    "save_matches(reconstructed2,test_kp2,test_path2, \"decoded_santo.txt\")\n",
    "save_matches(compressed2,test_kp2,test_path2, \"comp_santo.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute reconstruction losses\n",
    "ssd = []\n",
    "for des1, des2 in zip(test_des, reconstructed):\n",
    "    sd = np.mean((des1 - des2)**2)\n",
    "    ssd.append(sd)\n",
    "mse = np.mean(ssd)\n",
    "print (\"Reconstruction error for first test set\", mse)\n",
    "\n",
    "ssd2 = []\n",
    "for des1, des2 in zip(test_des2, reconstructed2):    \n",
    "    sd = np.mean((des1 - des2)**2)\n",
    "    ssd2.append(sd)\n",
    "mse2 = np.mean(ssd2)\n",
    "print (\"Reconstruction error for second test set\", mse2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPDe3hDgpgGPjivRAsdRdrw",
   "name": "",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
